{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full pipeline to learn the sheaf laplacian is going to take into account several steps. \n",
    "+ First of all, we take as granted the imputation of the maps via KKT on a given set of edges: we initialize the algorithm to work on the full graph starting from all maps being identity maps. \n",
    "+ We leverage structured dictionaries based on sheaf convolutional filters to learn the best representation of the 0-cochains with a double localization quality within each subdictionary: each atom gives first of all a node domain localization, and then a rank-1 stalk subspace localization. \n",
    "+ We then define a greedy criterion to start removing edges: at each iteration we compute all the possible representation of the 0-cochains removing one of the possible edges, and we decide to remove the edge yielding the best improvement on the sparse coding in terms of reconstruction error.\n",
    "+ Finally, once an edge has been removed, we compute all the restriction maps again and repeat the procedure until a convergence criterion is met. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "edges = []\n",
    "\n",
    "cutoff = 0.5\n",
    "theta = 0.9\n",
    "\n",
    "points = np.random.rand(N, 2)\n",
    "\n",
    "A = np.zeros((N,N))\n",
    "W = np.zeros((N,N))\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(i+1, N):\n",
    "        \n",
    "        A[i,j] = np.linalg.norm(points[i,:] - points[j,:]) <= cutoff\n",
    "        A[j,i] = np.linalg.norm(points[i,:] - points[j,:]) <= cutoff\n",
    "\n",
    "        if A[i,j] == 1:\n",
    "            edges.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sheaf(\n",
    "        V:int,\n",
    "        d:int,\n",
    "        edges:list\n",
    "        ) -> np.array:\n",
    "\n",
    "    E = len(edges)\n",
    "\n",
    "    # Incidency linear maps\n",
    "\n",
    "    F = {\n",
    "        e:{\n",
    "            e[0]:np.random.randn(d,d),\n",
    "            e[1]:np.random.randn(d,d)\n",
    "            } \n",
    "            for e in edges\n",
    "        }                                           \n",
    "\n",
    "    # Coboundary maps\n",
    "\n",
    "    B = np.zeros((d*E, d*V))                        \n",
    "\n",
    "    for i in range(len(edges)):\n",
    "\n",
    "        # Main loop to populate the coboundary map\n",
    "\n",
    "        edge = edges[i]\n",
    "\n",
    "        u = edge[0] \n",
    "        v = edge[1] \n",
    "\n",
    "        B_u = F[edge][u]\n",
    "        B_v = F[edge][v]\n",
    "\n",
    "        B[i*d:(i+1)*d, u*d:(u+1)*d] = B_u           \n",
    "        B[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "    L_f = B.T @ B\n",
    "\n",
    "    return L_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 5\n",
    "Lf = random_sheaf(N, d, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.diag(np.diagonal(Lf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lf = fractional_matrix_power(D, -0.5) @ Lf @ fractional_matrix_power(D, -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "band1 = np.array([i in range(0,25) for i in range(100)])\n",
    "band2 = np.array([i in range(25,50) for i in range(100)]) \n",
    "band3 = np.array([i in range(50,75) for i in range(100)])\n",
    "band4 = np.array([i in range(75,100) for i in range(100)])\n",
    "\n",
    "bands = [band1, band2, band3, band4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda, U = np.linalg.eig(Lf)\n",
    "Lambda = np.sort(Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 400\n",
    "DD = np.zeros((N*d, J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(J): \n",
    "\n",
    "    B = j // 100\n",
    "    h = np.copy(Lambda)\n",
    "    h[~bands[B]] = 0\n",
    "    h *= np.random.rand(h.shape[0],1)[:,0]\n",
    "    n = np.random.choice(d*N)\n",
    "\n",
    "    DD[:,j] = (U @ np.diag(h) @ U.T)[:,n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1000 \n",
    "Y_train = np.zeros((N*d,M))\n",
    "\n",
    "for m in range(M):\n",
    "    T = np.random.choice(J, 4)\n",
    "    Y_train[:,m] = DD[:,T] @ np.random.rand(4)\n",
    "\n",
    "test_size = 2000\n",
    "Y_test = np.zeros((N*d,test_size))\n",
    "\n",
    "for t in range(test_size):\n",
    "    T = np.random.choice(J, 4)\n",
    "    Y_test[:,t] = DD[:,T] @ np.random.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "eps = 0.01\n",
    "\n",
    "T0 = 20         # Assumed sparsity\n",
    "S = 4           # Number of dictionaries\n",
    "K = 5           # Power of the laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subroutine for sparse coding\n",
    "\n",
    "def OMP(y, D, T0):\n",
    "    S = []\n",
    "\n",
    "    x = np.zeros(D.shape[1])\n",
    "    iters = 0\n",
    "    R = y\n",
    "\n",
    "    while iters < T0:\n",
    "\n",
    "        # Retrieve the maximum correlation between atoms and residuals of the previous iteration\n",
    "        corrs = np.abs(D.T @ R)\n",
    "        corrs[S] = -np.inf\n",
    "        S.append(np.argmax(np.abs(D.T @ R)))\n",
    "\n",
    "        # Expand the dictionary for the representation\n",
    "        dic = D[:,S]\n",
    "\n",
    "        # Solve subproblems and update x\n",
    "        x[S] = np.linalg.inv(dic.T @ dic) @ dic.T @ y\n",
    "        \n",
    "        # Update the residuals\n",
    "        R = y - D @ x\n",
    "        iters += 1\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary update subroutine\n",
    "\n",
    "def LaplacianPowers(L, K):\n",
    "    powers = np.zeros((K+1, L.shape[0], L.shape[1]))\n",
    "    for k in range(K+1):\n",
    "        powers[k,:,:] = np.linalg.matrix_power(L, k)\n",
    "\n",
    "    return powers \n",
    "\n",
    "def DicUp(LaplPowers, K, S, Y, X, d, N, c, eps, mu = 1):\n",
    "    # Variables\n",
    "    alpha = cp.Variable((K+1, S))\n",
    "\n",
    "    # Define D_s expressions\n",
    "    D_s_list = []\n",
    "    for s in range(S):\n",
    "        D_s = sum(alpha[k, s] * LaplPowers[k,:,:] for k in range(K+1))\n",
    "        D_s_list.append(D_s)\n",
    "\n",
    "    # Objective function\n",
    "    D = cp.hstack(D_s_list)\n",
    "    objective = cp.Minimize(cp.norm(Y - D @ X, 'fro')**2 + mu * cp.norm(alpha, 2)**2)\n",
    "\n",
    "    # Constraints\n",
    "    constraints = []\n",
    "    for s in range(S):\n",
    "        constraints.append(D_s_list[s] >> 0)\n",
    "        constraints.append(D_s_list[s] << c * np.eye(d*N))\n",
    "\n",
    "    sum_D_s = sum(D_s_list)\n",
    "    constraints.append(sum_D_s >> (c - eps) * np.eye(d*N))\n",
    "    constraints.append(sum_D_s << (c + eps) * np.eye(d*N))\n",
    "\n",
    "    # Problem definition and solving\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.SCS, max_iters=100, eps=1e-5, warm_start=True)\n",
    "\n",
    "    alpha_star = alpha.value\n",
    "    D_s_list = []\n",
    "    for s in range(S):\n",
    "        D_s = sum(alpha_star[k, s] * LaplPowers[k,:,:] for k in range(K+1))\n",
    "        D_s_list.append(D_s)\n",
    "\n",
    "    # Objective function\n",
    "    D = np.hstack(D_s_list)\n",
    "    return D, alpha_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric dictionary learning algorithm \n",
    "\n",
    "def ParametricDictionaryLearning(Lf, Y, T0, K, S, d, N, c, eps, mu = 1, MAX_ITER = 5):\n",
    "\n",
    "    loss = []\n",
    "\n",
    "    # Precompute Laplacian powers\n",
    "    LP = LaplacianPowers(Lf, K)\n",
    "\n",
    "    # Dictionary initialization \n",
    "    D_s_list = []\n",
    "    for _ in range(S):\n",
    "        D_s = sum(LP[k,:,:] for k in range(K+1))\n",
    "        D_s_list.append(D_s)\n",
    "\n",
    "    D = np.hstack(D_s_list)\n",
    "\n",
    "    # Alternated learning procedure\n",
    "    for _ in tqdm(range(MAX_ITER)):\n",
    "        \n",
    "        # Sparse coding update\n",
    "        X = np.apply_along_axis(OMP, axis = 0, arr = Y, D = D, T0 = T0)\n",
    "\n",
    "        # Dictionary update\n",
    "        D, alpha_star = DicUp(LP, K, S, Y, X, d, N, c, eps, mu)\n",
    "        D = np.apply_along_axis(lambda x: x / np.linalg.norm(x), axis = 0, arr = D)\n",
    "\n",
    "        loss.append(np.linalg.norm(Y - D @ X, ord = 'fro'))\n",
    "        \n",
    "    return D, X, alpha_star, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedySelection(alpha, edges, d, Lf, K, Y, D, X):\n",
    "    \n",
    "    impros = {edge:None for edge in edges}\n",
    "\n",
    "    for edge in edges:\n",
    "\n",
    "        # Mask the sheaf laplacian assuming that such an edge is removed\n",
    "        L = np.copy(Lf)\n",
    "        u = edge[0]\n",
    "        v = edge[1]\n",
    "        L[u*d:(u+1)*d,v*d:(v+1)*d] = 0\n",
    "        L[v*d:(v+1)*d,u*d:(u+1)*d] = 0\n",
    "        LaplPowers = LaplacianPowers(L, K)\n",
    "\n",
    "        # Compute the retrieved dictionary \n",
    "        D_s_list = []\n",
    "        for s in range(S):\n",
    "            D_s = sum(alpha[k, s] * LaplPowers[k,:,:] for k in range(K+1))\n",
    "            D_s_list.append(D_s)\n",
    "\n",
    "        D_ = np.hstack(D_s_list)\n",
    "\n",
    "        # Store the improvement\n",
    "        impros[edge] = np.linalg.norm(Y - D @ X) - np.linalg.norm(Y - D_ @ X)\n",
    "\n",
    "    return [edge for edge in edges if edge != max(impros, key = impros.get)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def premultiplier(Xu, Xv):\n",
    "    uu = np.linalg.inv(Xu @ Xu.T)\n",
    "    uv = Xu @ Xv.T\n",
    "    vv = np.linalg.inv(Xv @ Xv.T)\n",
    "    vu = Xv @ Xu.T\n",
    "\n",
    "    return (uu, uv, vv, vu)\n",
    "\n",
    "def chi_u(uu, uv, vv, vu):\n",
    "\n",
    "    return ((uu @ uv - np.eye(uu.shape[0])) @ vv @ np.linalg.inv(vu @ uu @ uv @ vv - np.eye(uu.shape[0])) @ vu - np.eye(uu.shape[0])) @ uu\n",
    "\n",
    "def chi_v(uu, uv, vv, vu):\n",
    "\n",
    "    return (uu @ uv - np.eye(uu.shape[0])) @ vv @ np.linalg.inv(vu @ uu @ uv @ vv - np.eye(uu.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LaplacianUpdate(D, X, d, edges, N, Q):    \n",
    "    T = 0\n",
    "    Y = D @ X\n",
    "\n",
    "    maps = {\n",
    "        edge : {\n",
    "            edge[0] : np.zeros((d,d)),\n",
    "            edge[1] : np.zeros((d,d))\n",
    "        }\n",
    "    for edge in edges\n",
    "    }\n",
    "\n",
    "    for e in edges:\n",
    "        u = e[0]\n",
    "        v = e[1]\n",
    "\n",
    "        X_u = Y[u*d:(u+1)*d,:]\n",
    "        X_v = Y[v*d:(v+1)*d,:]\n",
    "        uu, uv, vv, vu = premultiplier(X_u, X_v)\n",
    "\n",
    "        maps[e][u] = chi_u(uu, uv, vv, vu)\n",
    "        maps[e][v] = - chi_v(uu, uv, vv, vu)\n",
    "        \n",
    "        T += np.trace(maps[e][u]) + np.trace(maps[e][v])\n",
    "\n",
    "    maps_ = {\n",
    "        edge : {\n",
    "            edge[0] : Q/T * maps[edge][edge[0]],\n",
    "            edge[1] : Q/T * maps[edge][edge[1]]\n",
    "        }\n",
    "        for edge in edges\n",
    "    }\n",
    "\n",
    "    Lf = np.zeros((d*N, d*N))\n",
    "    for edge in edges:\n",
    "\n",
    "        u = edge[0] \n",
    "        v = edge[1] \n",
    "\n",
    "        Lf[u*d:(u+1)*d,u*d:(u+1)*d] += maps_[edge][u].T @ maps_[edge][u]\n",
    "        Lf[v*d:(v+1)*d,v*d:(v+1)*d] += maps_[edge][v].T @ maps_[edge][v]\n",
    "        Lf[u*d:(u+1)*d,v*d:(v+1)*d] = - maps_[edge][u].T @ maps_[edge][v]\n",
    "        Lf[v*d:(v+1)*d,u*d:(u+1)*d] = - maps_[edge][v].T @ maps_[edge][u]\n",
    "\n",
    "    return Lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LaplacianLearningPipeline(\n",
    "        L,\n",
    "        Y, \n",
    "        E0,\n",
    "        T0, \n",
    "        K, \n",
    "        S, \n",
    "        d, \n",
    "        N, \n",
    "        c, \n",
    "        eps, \n",
    "        Q = 1,\n",
    "        mu = 1, \n",
    "        MAX_ITER = 5):\n",
    "    \n",
    "    # Initialization - Full graph with all identity maps\n",
    "    edges = list(combinations(range(N),2))\n",
    "    B = np.zeros((d*len(edges), d*N))                        \n",
    "\n",
    "    for i in range(len(edges)):\n",
    "\n",
    "        # Main loop to populate the coboundary map\n",
    "        edge = edges[i]\n",
    "\n",
    "        u = edge[0] \n",
    "        v = edge[1] \n",
    "\n",
    "        B_u = np.eye(d)\n",
    "        B_v = np.eye(d)\n",
    "\n",
    "        B[i*d:(i+1)*d, u*d:(u+1)*d] = B_u           \n",
    "        B[i*d:(i+1)*d, v*d:(v+1)*d] = - B_v\n",
    "\n",
    "    Lf = B.T @ B\n",
    "    Lf = fractional_matrix_power(np.diag(np.diagonal(Lf)), -0.5) @ Lf @ fractional_matrix_power(np.diag(np.diagonal(Lf)), -0.5)\n",
    "\n",
    "    # Main loop - First try with a priori knowledge of the number of connections\n",
    "\n",
    "    while len(edges) > E0:\n",
    "        \n",
    "        # Dictionary learning phase\n",
    "        D, X, alpha, _ = ParametricDictionaryLearning(Lf, Y, T0, K, S, d, N, c, eps, mu, MAX_ITER)\n",
    "\n",
    "        # Greedy deleting one edge from the graph\n",
    "        edges = GreedySelection(alpha, edges, d, Lf, K, Y, D, X)\n",
    "\n",
    "        # Rebuilding the sheaf laplacian on the new graph\n",
    "        Lf = LaplacianUpdate(D, X, d, edges, N, Q)\n",
    "        Lf = fractional_matrix_power(np.diag(np.diagonal(Lf)), -0.5) @ Lf @ fractional_matrix_power(np.diag(np.diagonal(Lf)), -0.5)\n",
    "        print(np.linalg.norm(L - Lf) / L.size)\n",
    "        \n",
    "    return Lf, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "E0 = len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_hat, edges_ = LaplacianLearningPipeline(Lf, Y_train, E0, T0, K, S, d, N, c, eps, MAX_ITER=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -1.23688641e-01, -2.12171637e-01, ...,\n",
       "         2.11066775e-02, -3.50020302e-02,  6.58189308e-02],\n",
       "       [-1.23688641e-01,  1.00000000e+00, -1.06094094e-01, ...,\n",
       "         8.32318058e-03, -8.50513066e-03,  4.05869278e-04],\n",
       "       [-2.12171637e-01, -1.06094094e-01,  1.00000000e+00, ...,\n",
       "        -1.07284866e-01,  1.06047133e-02, -1.41085287e-02],\n",
       "       ...,\n",
       "       [ 2.11066775e-02,  8.32318058e-03, -1.07284866e-01, ...,\n",
       "         1.00000000e+00,  5.02674574e-02, -2.17200542e-02],\n",
       "       [-3.50020302e-02, -8.50513066e-03,  1.06047133e-02, ...,\n",
       "         5.02674574e-02,  1.00000000e+00, -1.60690784e-01],\n",
       "       [ 6.58189308e-02,  4.05869278e-04, -1.41085287e-02, ...,\n",
       "        -2.17200542e-02, -1.60690784e-01,  1.00000000e+00]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDG = set(edges)\n",
    "EDG_HAT = set(edges_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5894736842105263"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(EDG.intersection(EDG_HAT))/E0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big space for improvement but best result so far!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
